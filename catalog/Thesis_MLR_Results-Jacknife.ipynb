{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "import ujson as json\n",
    "import calendar\n",
    "import datetime, time\n",
    "import time as t\n",
    "import scipy\n",
    "import numpy as np\n",
    "start=t.time()\n",
    "filename='Cedar_Valley.json'\n",
    "with open(filename, 'r') as f:\n",
    "    points = json.load(f)\n",
    "print len(points['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found him\n",
      "8.25202393532\n",
      "Well  373236113111401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/tethys/miniconda/envs/tethys/lib/python2.7/site-packages/ipykernel_launcher.py:100: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/student/tethys/miniconda/envs/tethys/lib/python2.7/site-packages/ipykernel_launcher.py:108: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/student/tethys/miniconda/envs/tethys/lib/python2.7/site-packages/ipykernel_launcher.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/student/tethys/miniconda/envs/tethys/lib/python2.7/site-packages/ipykernel_launcher.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR\n",
      "-49.943681272142946\n",
      "Actual\n",
      "-47.56854331861177\n",
      "finished\n",
      "9.19168496132\n"
     ]
    }
   ],
   "source": [
    "jacknife=373236113111401\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_date=1985\n",
    "end_date=2015\n",
    "interval=5\n",
    "resolution=.01\n",
    "time_tolerance=5\n",
    "min_ratio=1\n",
    "min_samples=5\n",
    "iterations=int((end_date-start_date)/interval+1)\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "pd.options.display.max_rows=1000\n",
    "pp = PdfPages('multipage.pdf')\n",
    "firststart=t.time()\n",
    "interpolation_df=pd.DataFrame()\n",
    "combined_df = pd.DataFrame()\n",
    "for well in points['features']:\n",
    "    if well['properties']['HydroID']==jacknife or well['properties']['HydroID']==str(jacknife):\n",
    "        print(\"found him\")\n",
    "        jacklon=well['geometry']['coordinates'][0]\n",
    "        jacklat=well['geometry']['coordinates'][1]\n",
    "    if 'TsTime' in well:\n",
    "        if len(well['TsTime'])>min_samples and (np.array(well['TsTime']).max()-np.array(well['TsTime']).min())>pd.Timedelta(days=365*5).value/1000000000:\n",
    "            if np.array(well['TsTime']).max()>calendar.timegm(datetime.datetime(end_date, 1, 1).timetuple()) and np.array(well['TsTime']).min()<calendar.timegm(datetime.datetime(start_date, 1, 1).timetuple()):\n",
    "                name = str(well['properties']['HydroID'])\n",
    "            else:\n",
    "                name = str(well['properties']['HydroID']) + 'Short'\n",
    "            wells_df = pd.DataFrame(index=pd.to_datetime(well['TsTime'], unit='s', origin='unix'), data=well['TsValue'], columns=[name])\n",
    "            wells_df.index.drop_duplicates(keep=\"first\")\n",
    "            wells_df = wells_df.resample('3M').mean()\n",
    "\n",
    "            combined_df = pd.concat([combined_df, wells_df], join=\"outer\", axis=1)\n",
    "            combined_df.drop_duplicates(inplace=True)\n",
    "#combined_df is a pandas dataframe containing the ts depth to water table values for each well in the aquifer with the min number of points\n",
    "# The columns of combined_df are the HydroIDs of the wells. If the well timeseries data spans the period of interpolation, the column title\n",
    "# is the HydroID. If the timeseries does not span the time period, then the column title is the HydroID + \"Short\"\n",
    "# The rows of the database are the timeseries values for 3 month intervals from start_date to end_date\n",
    "corrs_df = combined_df.interpolate(method='pchip', limit_area='inside', limit=8)\n",
    "combined_df.interpolate(method='pchip', inplace=True, limit_area='inside')\n",
    "\n",
    "#This code resamples the data so we are working on March data since that is what Utah records\n",
    "combined_df=combined_df.resample('Q').nearest()\n",
    "decemb=combined_df.index.get_loc(str(start_date)+'-12-31 00:00:00',method='nearest') % 4\n",
    "combined_df=combined_df.iloc[decemb+1::4]\n",
    "\n",
    "combined_df['linear']=np.linspace(0,1,len(combined_df))\n",
    "interpolation_df = combined_df.drop(combined_df.filter(regex='Short').columns, axis=1)\n",
    "corr_df=corrs_df.corr(min_periods=12)#12\n",
    "corr_df=corr_df-np.identity(len(corr_df))\n",
    "length=len(corr_df)-1\n",
    "for row in corr_df.index:\n",
    "    if 'Short' in str(row):\n",
    "        corr_df=corr_df.drop(row)\n",
    "print t.time()-firststart\n",
    "\n",
    "well_list=[]\n",
    "for i in range(length):#length\n",
    "    reflist=np.array(corr_df.nlargest(5,corr_df.columns[i]).index)\n",
    "    welli = corr_df.columns[i]\n",
    "    well_list.append(welli)\n",
    "    if str(jacknife) in str(welli):\n",
    "        mydf = combined_df[[welli]].copy()\n",
    "        print \"Well \",welli\n",
    "        corr_values=corr_df.nlargest(5, corr_df.columns[i]).values[:,i]\n",
    "        reflist=np.append(reflist,'linear')\n",
    "        delete_list=[]\n",
    "        for j in range(len(corr_values)):\n",
    "            if corr_values[j]<=0.5:\n",
    "                delete_list.append(j)\n",
    "        if len(delete_list)>0:\n",
    "            reflist=np.delete(reflist,delete_list)\n",
    "        if len(reflist)>=1:\n",
    "            for ref in reflist:\n",
    "                mydf=pd.concat([mydf,combined_df[ref]], axis=1)\n",
    "            norm_exdf=mydf.dropna(how=\"any\", subset=reflist)\n",
    "            normz_exdf=norm_exdf.copy()\n",
    "            smaller_exdf=normz_exdf.copy()\n",
    "            smaller_exdf=smaller_exdf.dropna(how=\"any\")\n",
    "            for column in norm_exdf.columns[0:]:\n",
    "                normz_exdf[column]=(norm_exdf[column]-smaller_exdf[column].min())/(smaller_exdf[column].max()-smaller_exdf[column].min())\n",
    "            exdf = normz_exdf.copy()\n",
    "            mymin=max(exdf[welli].first_valid_index(),exdf[reflist[0]].first_valid_index())\n",
    "            mymax=\"1995-01-01 00:00:00\"\n",
    "            exdf=exdf[mymin:mymax]\n",
    "            y=exdf[welli]\n",
    "            x=exdf.as_matrix(columns=reflist)\n",
    "            n,m=x.shape\n",
    "            x0=np.ones((n,1))\n",
    "            x=np.hstack((x,x0))\n",
    "            model=sm.OLS(y,x)\n",
    "            model_fit = model.fit_regularized(alpha=0.005)\n",
    "            b=np.array(model_fit.params)\n",
    "\n",
    "            A=normz_exdf.as_matrix(columns=reflist)\n",
    "            n,m=A.shape\n",
    "            A0=np.ones((n,1))\n",
    "            A=np.hstack((A,A0))\n",
    "            norm_exdf['prediction']=np.dot(A,b)\n",
    "            norm_exdf['prediction']=norm_exdf['prediction']*(norm_exdf[welli].max()-norm_exdf[welli].min())+norm_exdf[welli].min()\n",
    "            #toggle this next line of code\n",
    "#             norm_exdf.loc[corrs_df[welli].notnull(),'prediction']=corrs_df[welli]\n",
    "            newname=str(welli).replace('Short','')\n",
    "            interpolation_df = pd.concat([interpolation_df, norm_exdf['prediction']], join=\"outer\", axis=1)\n",
    "            interpolation_df=interpolation_df.rename(columns={\"prediction\":newname})\n",
    "            plt.plot(combined_df[welli], 'r', label='Original Data')\n",
    "            plt.plot(norm_exdf['prediction'], 'r--', label='MLR Prediction')\n",
    "            plt.plot(combined_df[welli][mymin:mymax], 'g', label='Training Data')\n",
    "            combined_df[welli+'naive']=combined_df[welli]\n",
    "            combined_df[welli+'naive'][mymax:]=combined_df[welli+'naive'][combined_df.index.get_loc(mymax,method='nearest')]\n",
    "            combined_df[welli+'prediction']=norm_exdf['prediction']\n",
    "            if \"Short\" in welli:\n",
    "                well_title=\"Well \"+welli[:-5]\n",
    "            else:\n",
    "                well_title=\"Well \"+welli\n",
    "            plt.title(well_title)\n",
    "            plt.legend()\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Depth to GW (ft)')\n",
    "            pp.savefig()\n",
    "            plt.close()\n",
    "            test_index=combined_df.index.get_loc('2015-01-01 00:00:00',method='nearest')\n",
    "            print(\"MLR\")\n",
    "            print(combined_df[welli+'prediction'][test_index])\n",
    "            print(\"Actual\")\n",
    "            print(combined_df[welli][test_index])\n",
    "pp.close()\n",
    "plt.show()\n",
    "print \"finished\"\n",
    "print t.time()-firststart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newinterpolation_df=interpolation_df[str(start_date):str(end_date)].resample('5Y',closed='left').nearest()\n",
    "corrs_df=corrs_df[str(start_date):str(end_date)].resample('5Y',closed='left').nearest()\n",
    "corrs_df.rename(columns=lambda x: x.replace('Short',''),inplace=True)\n",
    "lons=[]\n",
    "lats=[]\n",
    "values=[]\n",
    "elevations=[]\n",
    "ids=[]\n",
    "mylon=[]\n",
    "mylat=[]\n",
    "myelevs=[]\n",
    "myids=[]\n",
    "for wellid in newinterpolation_df.columns:\n",
    "    for well in points['features']:\n",
    "        if wellid==str(well['properties']['HydroID']):\n",
    "            mylon.append(well['geometry']['coordinates'][0])\n",
    "            mylat.append(well['geometry']['coordinates'][1])\n",
    "            myelevs.append(well['properties']['LandElev'])\n",
    "            myids.append(wellid)\n",
    "            newinterpolation_df.loc[corrs_df[wellid].notnull(),wellid]=corrs_df[wellid]\n",
    "            break\n",
    "for i in range(iterations):\n",
    "    myvalue=np.array(newinterpolation_df.iloc[i].tolist())\n",
    "    myelev=np.add(np.array(myelevs),myvalue)\n",
    "    lons.append(mylon)\n",
    "    lats.append(mylat)\n",
    "    values.append(myvalue)\n",
    "    elevations.append(myelev)  \n",
    "    ids.append(myids)\n",
    "    print len(mylon)\n",
    "lons = np.array(lons)\n",
    "lats = np.array(lats)\n",
    "ids=np.array(ids)\n",
    "values = np.array(values)\n",
    "elevations = np.array(elevations)\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nlags=6\n",
    "#Variogram\n",
    "def distance(lon1,lat1,lon2,lat2):\n",
    "    d2=(lon2-lon1)**2+(lat2-lat1)**2\n",
    "    d=d2**(.5)\n",
    "    return d\n",
    "def deviation(value1,value2):\n",
    "    diff=(value1-value2)**2\n",
    "    return diff\n",
    "\n",
    "all_distances=[]\n",
    "all_deviations=[]\n",
    "for step in range(0,len(lons)):\n",
    "    distances=[]\n",
    "    deviations=[]\n",
    "    lon=lons[step]\n",
    "    lat=lats[step]\n",
    "    value=values[step]\n",
    "    elevation=elevations[step]\n",
    "    for x in range(0,len(lon)):\n",
    "        for y in range(0,len(lon)):\n",
    "            if x==y:\n",
    "                continue\n",
    "            myx=distance(lon[x],lat[x],lon[y],lat[y])\n",
    "            myy=deviation(value[x],value[y])\n",
    "            distances.append(myx)\n",
    "            deviations.append(myy)\n",
    "    distances=np.array(distances)\n",
    "    deviations=np.array(deviations)\n",
    "    max=np.max(distances)\n",
    "    min=np.min(distances)\n",
    "    size=((max-min)/4)/nlags\n",
    "\n",
    "    xbins=[]\n",
    "    ybins=[]\n",
    "    meanx=[]\n",
    "    meany=[]\n",
    "    \n",
    "    for c in range(0,(nlags*4)):\n",
    "        xbin1=[]\n",
    "        ybin1=[]\n",
    "        for d in range(0,len(distances)):\n",
    "            if distances[d]>=(min+size*c) and distances[d]<(min+size*(c+1)):\n",
    "                xbin1.append(distances[d])\n",
    "                ybin1.append(deviations[d])\n",
    "        xbin1=np.array(xbin1)\n",
    "        ybin1=np.array(ybin1)\n",
    "        meanx1=np.average(xbin1)\n",
    "        meany1=np.average(ybin1)\n",
    "        xbins.append(xbin1)\n",
    "        ybins.append(ybin1)\n",
    "        meanx.append(meanx1)\n",
    "        meany.append(meany1/2)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "#     plt.scatter(distances, deviations, label='Data')\n",
    "    plt.scatter(meanx,meany,label='Average')\n",
    "    plt.show() \n",
    "    all_distances.append(distances)\n",
    "    all_deviations.append(deviations)\n",
    "all_distances=np.array(all_distances)\n",
    "all_deviations=np.array(all_distances)\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lonmin=360.0\n",
    "latmin=90.0\n",
    "lonmax=-360.0\n",
    "latmax=-90.0\n",
    "for i in points['features']:\n",
    "    if i['geometry']['coordinates'][0]<lonmin:\n",
    "        lonmin=i['geometry']['coordinates'][0]\n",
    "    if i['geometry']['coordinates'][0]>lonmax:\n",
    "        lonmax=i['geometry']['coordinates'][0]\n",
    "    if i['geometry']['coordinates'][1]<latmin:\n",
    "        latmin=i['geometry']['coordinates'][1]\n",
    "    if i['geometry']['coordinates'][1]>latmax:\n",
    "        latmax=i['geometry']['coordinates'][1]\n",
    "lonmin=round(lonmin-.05,1)\n",
    "latmin=round(latmin-.05,1)\n",
    "lonmax=round(lonmax+.05,1)\n",
    "latmax=round(latmax+.05,1)\n",
    "print lonmin,lonmax,latmin,latmax\n",
    "print len(points['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "latgrid = np.mgrid[latmin:latmax:resolution]\n",
    "longrid = np.mgrid[lonmin:lonmax:resolution]\n",
    "latrange=len(latgrid)\n",
    "lonrange=len(longrid)\n",
    "print latrange, lonrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pygslib\n",
    "outx=np.repeat(longrid,latrange)\n",
    "outy=np.tile(latgrid,lonrange)\n",
    "grids=[]\n",
    "for i in range(0,iterations):\n",
    "    params={\n",
    "        'x':lons[i],\n",
    "        'y':lats[i],\n",
    "        'vr':values[i],\n",
    "        'nx':lonrange,\n",
    "        'ny':latrange,\n",
    "        'nz':1,\n",
    "        'xmn':lonmin,\n",
    "        'ymn':latmin,\n",
    "        'zmn':0,\n",
    "        'xsiz':resolution,\n",
    "        'ysiz':resolution,\n",
    "        'zsiz':1,\n",
    "        'nxdis':1,\n",
    "        'nydis':1,\n",
    "        'nzdis':1,\n",
    "        'outx':outx,\n",
    "        'outy':outy,\n",
    "        'radius':2,\n",
    "        'radius1':2,\n",
    "        'radius2':2,\n",
    "        'ndmax':15,\n",
    "        'ndmin':5,\n",
    "        'noct':0,\n",
    "        'ktype':1,\n",
    "        'idbg':0,\n",
    "        'c0':0,\n",
    "        'it':1,\n",
    "        'cc':500,\n",
    "        'aa':.2,\n",
    "        'aa1':.2,\n",
    "        'aa2':.2\n",
    "    }\n",
    "    estimate= pygslib.gslib.kt3d(params)\n",
    "    print i\n",
    "    array=estimate[0]['outidpower']#outest\n",
    "    array2=estimate[0]['outidpower']\n",
    "    grid=np.reshape(array,(lonrange,latrange))\n",
    "    grid2=np.reshape(array2,(lonrange,latrange))\n",
    "    x=np.isnan(grid)\n",
    "    grid[x]=grid2[x]\n",
    "    grids.append(grid)\n",
    "grids=np.array(grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('UtahMajorAquifers.json','r') as f:\n",
    "    allwells = ''\n",
    "    wells = f.readlines()\n",
    "    for i in range(0, len(wells)):\n",
    "        allwells += wells[i]\n",
    "utah=json.loads(allwells)\n",
    "\n",
    "\n",
    "AquiferShape= {\n",
    "    'type':'FeatureCollection',\n",
    "    'features':[]\n",
    "}\n",
    "print utah['features'][0].viewkeys()\n",
    "for i in utah['features']:\n",
    "    if i['properties']['Full_Name']==\"Cedar Valley\":#Full_Name #AQ_NAME TRINITY\n",
    "        AquiferShape['features'].append(i)\n",
    "\n",
    "with open('shapefile.json','w') as outfile:\n",
    "    json.dump(AquiferShape, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import netCDF4\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import time as t\n",
    "from scipy import interpolate\n",
    "from sklearn.utils import resample\n",
    "import pykrige.kriging_tools as kt\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cython\n",
    "hydros= np.array(ids[0])\n",
    "volunit=\"Acre-ft\"\n",
    "\n",
    "interpolation_type=\"Kriging\"\n",
    "# f=idw(spots,values)\n",
    "# print f([-96.8,32.7],power=1.0)\n",
    "# f([-96.0,30.7],power=2,n_neighbors=len(values))\n",
    "\n",
    "latlen=len(latgrid)\n",
    "lonlen=len(longrid)\n",
    "filename='testinterpolate2.nc'\n",
    "h=netCDF4.Dataset(filename,'w',format=\"NETCDF4\")\n",
    "h.interpolation=\"Kriging Multi Linear\"\n",
    "h.start_date=start_date\n",
    "h.end_date=end_date\n",
    "h.interval=interval\n",
    "h.resolution=resolution\n",
    "h.min_samples=min_samples\n",
    "h.min_ratio=min_ratio\n",
    "h.time_tolerance=time_tolerance\n",
    "h.default=0\n",
    "h.units='English'\n",
    "\n",
    "\n",
    "time = h.createDimension(\"time\", 0)\n",
    "lat = h.createDimension(\"lat\", latlen)\n",
    "lon = h.createDimension(\"lon\", lonlen)\n",
    "hydroid=h.createDimension(\"hydroid\", len(ids[0]))\n",
    "latitude=h.createVariable(\"lat\",np.float64,(\"lat\"))\n",
    "longitude=h.createVariable(\"lon\",np.float64,(\"lon\"))\n",
    "time=h.createVariable(\"time\",np.float64,(\"time\"), fill_value=\"NaN\")\n",
    "hydroids=h.createVariable(\"hydroid\",str,(\"hydroid\"), fill_value=\"NaN\")\n",
    "hydroids.axis=\"H\"\n",
    "\n",
    "tsvalue=h.createVariable(\"tsvalue\",np.float64,('time','hydroid'),fill_value=-9999)\n",
    "tsvalue.units=\"ft\"\n",
    "tsvalue.coordinates=\"time hydroid\"\n",
    "\n",
    "depth=h.createVariable(\"depth\",np.float64,('time','lon','lat'),fill_value=-999)\n",
    "depth.long_name=\"Depth to Groundwater\"\n",
    "depth.units=\"ft\"\n",
    "depth.grid_mapping=\"WGS84\"\n",
    "depth.cell_measures=\"area: area\"\n",
    "depth.coordinates=\"time lat lon\"\n",
    "\n",
    "elevation=h.createVariable(\"elevation\",np.float64,('time','lon','lat'),fill_value=-999)\n",
    "elevation.long_name=\"Elevation of Groundwater\"\n",
    "elevation.units=\"ft\"\n",
    "elevation.grid_mapping=\"WGS84\"\n",
    "elevation.cell_measures=\"area: area\"\n",
    "elevation.coordinates=\"time lat lon\"\n",
    "\n",
    "drawdown=h.createVariable(\"drawdown\",np.float64,('time','lon','lat'),fill_value=-999)\n",
    "drawdown.long_name=\"Well Drawdown\"\n",
    "drawdown.units=\"ft\"\n",
    "drawdown.grid_mapping=\"WGS84\"\n",
    "drawdown.cell_measures=\"area: area\"\n",
    "drawdown.coordinates=\"time lat lon\"\n",
    "\n",
    "volume = h.createVariable(\"volume\", np.float64, ('time', 'lon', 'lat'), fill_value=-9999)\n",
    "volume.long_name = \"Change in aquifer storage volume since \" + str(start_date)\n",
    "volume.units = volunit\n",
    "volume.grid_mapping = \"WGS84\"\n",
    "volume.cell_measures = \"area: area\"\n",
    "volume.coordinates = \"time lat lon\"\n",
    "\n",
    "latitude.long_name=\"Latitude\"\n",
    "latitude.units=\"degrees_north\"\n",
    "latitude.axis=\"Y\"\n",
    "longitude.long_name=\"Longitude\"\n",
    "longitude.units=\"degrees_east\"\n",
    "longitude.axis=\"X\"\n",
    "time.axis=\"T\"\n",
    "time.units='days since 0001-01-01 00:00:00 UTC'\n",
    "latitude[:]=latgrid[:]\n",
    "longitude[:]=longrid[:]\n",
    "hydroids[:]=ids[0,:]\n",
    "year=start_date\n",
    "timearray=[]#[datetime.datetime(2000,1,1).toordinal()-1,datetime.datetime(2002,1,1).toordinal()-1]\n",
    "            \n",
    "for i in range(0,iterations):\n",
    "    print i\n",
    "    timearray.append(datetime.datetime(year, 1, 1).toordinal() - 1)\n",
    "    year += interval\n",
    "    time[i] = timearray[i]\n",
    "    tsvalue[i,:]=values[i,:]\n",
    "    for x in range(0,lonrange):\n",
    "            depth[i,x,:]=grids[i,x,:]\n",
    "            elevation[i,x,:]=grids[i,x,:]\n",
    "            if i==0:\n",
    "                drawdown[i,x,:]=0\n",
    "            else:\n",
    "                drawdown[i,x,:]=depth[i,x,:]-depth[0,x,:]\n",
    "\n",
    "h.close()\n",
    "os.system(\"./aquifersubset.sh %s\" % (filename))\n",
    "# serverpath=\"/home/student/tds/apache-tomcat-8.5.30/content/thredds/public/testdata/groundwater\"\n",
    "# destination= os.path.join(serverpath, filename)\n",
    "# os.rename(filename, destination)\n",
    "end=t.time()\n",
    "print(end-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.system(\"./aquifersubset.sh %s\" % (filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import copy\n",
    "from xml.etree import cElementTree as ET\n",
    "# This function returns the first and last available time from a url of a getcapabilities page located on a Thredds Server\n",
    "def getTimeBounds(url):\n",
    "    f=urllib.urlopen(url)\n",
    "    tree=ET.parse(f)\n",
    "    root=tree.getroot()\n",
    "    #These lines of code find the time dimension information for the netcdf on the Thredds server\n",
    "    dim=root.findall('.//{http://www.opengis.net/wms}Dimension')\n",
    "    dim=dim[0].text\n",
    "    times=dim.split(',')\n",
    "    times.pop(0)\n",
    "    timemin=times[0]\n",
    "    timemax=times[-1]\n",
    "    #timemin and timemax are the first and last available times on the specified url\n",
    "    return timemin,timemax\n",
    "#This function returns a pandas dataframe of the timeseries values of a specific layer \n",
    "#at a specific latitude and longitude from a file on a Thredds server\n",
    "#server: the url of the netcdf desired netcdf file on the Thredds server to read\n",
    "#layer: the name of the layer to extract timeseries information from for the netcdf file\n",
    "#lat: the latitude of the point at which to extract the timeseries\n",
    "#lon: the longitude of the point at which to extract the timeseries\n",
    "#returns df: a pandas dataframe of the timeseries at lat and lon for the layer in the server netcdf file\n",
    "def getThreddsValue(server,layer,lat,lon):\n",
    "    #calls the getTimeBounds function to get the first and last available times for the netcdf file on the server\n",
    "    timemin,timemax=getTimeBounds(server+\"?service=WMS&version=1.3.0&request=GetCapabilities\")\n",
    "    #These lines properly format a url request for the timeseries of a speific layer from a netcdf on \n",
    "    #a Thredds server\n",
    "    server=server+\"?service=WMS&version=1.3.0&request=GetFeatureInfo&CRS=CRS:84&QUERY_LAYERS=\"+layer\n",
    "    server=server+\"&X=0&Y=0&I=0&J=0&BBOX=\"+str(lon)+','+str(lat)+','+str(lon+.001)+','+str(lat+.001)\n",
    "    server=server+\"&WIDTH=1&Height=1&INFO_FORMAT=text/xml\"\n",
    "    server=server+'&TIME='+timemin+'/'+timemax\n",
    "    f = urllib.urlopen(server)\n",
    "    tree=ET.parse(f)\n",
    "    root=tree.getroot()\n",
    "    features=root.findall('FeatureInfo')\n",
    "    times=[]\n",
    "    values=[]\n",
    "    for child in features:\n",
    "        time=datetime.datetime.strptime(child[0].text,\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        times.append(time)\n",
    "        values.append(child[1].text)\n",
    "    \n",
    "    df=pd.DataFrame(index=times, columns=[layer],data=values)\n",
    "    df[layer]=df[layer].replace('none',np.nan).astype(float)\n",
    "    return df\n",
    "\n",
    "#function to convert a datetime object to milliseconds since epoch\n",
    "def datetime_to_float(d):\n",
    "    epoch = datetime.datetime.utcfromtimestamp(0)\n",
    "    total_seconds =  (d - epoch).total_seconds()\n",
    "    # total_seconds will be in decimals (millisecond precision)\n",
    "    return total_seconds\n",
    "\n",
    "\n",
    "server=\"http://localhost:8080/thredds/wms/testAll/groundwater/testinterpolate2.nc\"\n",
    "layer=\"depth\"\n",
    "#calls a function to return a df of the timeseries of the pdsi at 37.7,-113\n",
    "df1=getThreddsValue(server,layer,jacklat,jacklon)\n",
    "print(df1)\n",
    "print(jacklat,jacklon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
